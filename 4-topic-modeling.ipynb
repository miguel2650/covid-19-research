{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"data = pd.read_pickle('pickle/data_dtm.pkl')\\ndata\""
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Let's read in our document-term matrix\n",
    "import pandas as pd\n",
    "import pickle\n",
    "\n",
    "data = pd.read_pickle('pickle/data_dtm.pkl')\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'# One of the required inputs is a term-document matrix\\ntdm = data.transpose()\\ntdm.head()'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Import the necessary modules for LDA with gensim\n",
    "from gensim import matutils, models\n",
    "import scipy.sparse\n",
    "\n",
    "# One of the required inputs is a term-document matrix\n",
    "tdm = data.transpose()\n",
    "tdm.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We're going to put the term-document matrix into a new gensim format, from df --> sparse matrix --> gensim corpus\n",
    "sparse_counts = scipy.sparse.csr_matrix(tdm)\n",
    "corpus = matutils.Sparse2Corpus(sparse_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv = pickle.load(open(\"cv_stop.pkl\", \"rb\"))\n",
    "id2word = dict((v, k) for k, v in cv.vocabulary_.items())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0,\n",
       "  '0.025*\"said\" + 0.010*\"coronavirus\" + 0.010*\"million\" + 0.008*\"company\" + 0.008*\"new\" + 0.007*\"year\" + 0.007*\"pandemic\" + 0.007*\"billion\" + 0.006*\"economy\" + 0.006*\"economic\"'),\n",
       " (1,\n",
       "  '0.018*\"said\" + 0.012*\"coronavirus\" + 0.012*\"people\" + 0.007*\"time\" + 0.007*\"health\" + 0.007*\"new\" + 0.005*\"day\" + 0.005*\"newspaper\" + 0.005*\"just\" + 0.005*\"dont\"')]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Now that we have the corpus (term-document matrix) and id2word (dictionary of location: term),\n",
    "# we need to specify two other parameters as well - the number of topics and the number of passes\n",
    "lda = models.LdaModel(corpus=corpus, id2word=id2word, num_topics=2, passes=10)\n",
    "lda.print_topics()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0,\n",
       "  '0.040*\"said\" + 0.018*\"coronavirus\" + 0.010*\"new\" + 0.008*\"government\" + 0.007*\"pandemic\" + 0.007*\"trump\" + 0.007*\"economy\" + 0.006*\"week\" + 0.006*\"china\" + 0.006*\"million\"'),\n",
       " (1,\n",
       "  '0.013*\"said\" + 0.013*\"people\" + 0.011*\"coronavirus\" + 0.008*\"time\" + 0.007*\"newspaper\" + 0.006*\"dont\" + 0.006*\"health\" + 0.006*\"just\" + 0.005*\"day\" + 0.005*\"home\"'),\n",
       " (2,\n",
       "  '0.016*\"company\" + 0.009*\"million\" + 0.008*\"business\" + 0.007*\"new\" + 0.007*\"financial\" + 0.006*\"data\" + 0.006*\"year\" + 0.005*\"time\" + 0.005*\"information\" + 0.004*\"revenue\"')]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# LDA for num_topics = 3\n",
    "lda = models.LdaModel(corpus=corpus, id2word=id2word, num_topics=3, passes=10)\n",
    "lda.print_topics()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0,\n",
       "  '0.013*\"company\" + 0.012*\"million\" + 0.011*\"market\" + 0.010*\"year\" + 0.009*\"billion\" + 0.009*\"oil\" + 0.009*\"said\" + 0.008*\"quarter\" + 0.008*\"business\" + 0.008*\"financial\"'),\n",
       " (1,\n",
       "  '0.044*\"said\" + 0.018*\"coronavirus\" + 0.012*\"government\" + 0.010*\"new\" + 0.008*\"people\" + 0.008*\"trump\" + 0.007*\"pandemic\" + 0.007*\"state\" + 0.007*\"crisis\" + 0.007*\"week\"'),\n",
       " (2,\n",
       "  '0.010*\"time\" + 0.009*\"newspaper\" + 0.008*\"just\" + 0.007*\"dont\" + 0.007*\"said\" + 0.007*\"like\" + 0.006*\"coronavirus\" + 0.005*\"use\" + 0.005*\"people\" + 0.005*\"going\"'),\n",
       " (3,\n",
       "  '0.019*\"people\" + 0.016*\"coronavirus\" + 0.016*\"health\" + 0.015*\"said\" + 0.011*\"virus\" + 0.008*\"care\" + 0.007*\"testing\" + 0.007*\"new\" + 0.006*\"day\" + 0.006*\"hospital\"')]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# LDA for num_topics = 4\n",
    "lda = models.LdaModel(corpus=corpus, id2word=id2word, num_topics=4, passes=10)\n",
    "lda.print_topics()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/tobiaskarentiuskromanndahl/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /Users/tobiaskarentiuskromanndahl/nltk_data...\n",
      "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n"
     ]
    }
   ],
   "source": [
    "# Create a function to pull out nouns from a string of text\n",
    "from nltk import word_tokenize, pos_tag\n",
    "import nltk\n",
    "\n",
    "nltk.download('punkt')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "\n",
    "def nouns(text):\n",
    "    '''Given a string of text, tokenize the text and pull out only the nouns.'''\n",
    "    is_noun = lambda pos: pos[:2] == 'NN'\n",
    "    tokenized = word_tokenize(text)\n",
    "    all_nouns = [word for (word, pos) in pos_tag(tokenized) if is_noun(pos)] \n",
    "    return ' '.join(all_nouns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>content</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>so sorry to tell you that you have stage ovari...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>if you buy something through link on this page...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>cluster of more than pneumonia in the central ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>at hong international airport are being for my...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>the finding that the outbreak of viral pneumon...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>55532</th>\n",
       "      <td>good morning said the government had only ever...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>55533</th>\n",
       "      <td>forward his future at despite talk that is con...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>55534</th>\n",
       "      <td>the deal agreed by new jersey new york and isl...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>55535</th>\n",
       "      <td>around the world have seen major and amid the ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>55536</th>\n",
       "      <td>smoker who dealt with pancreatitis and cancer ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>55537 rows Ã— 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 content\n",
       "0      so sorry to tell you that you have stage ovari...\n",
       "1      if you buy something through link on this page...\n",
       "2      cluster of more than pneumonia in the central ...\n",
       "3      at hong international airport are being for my...\n",
       "4      the finding that the outbreak of viral pneumon...\n",
       "...                                                  ...\n",
       "55532  good morning said the government had only ever...\n",
       "55533  forward his future at despite talk that is con...\n",
       "55534  the deal agreed by new jersey new york and isl...\n",
       "55535  around the world have seen major and amid the ...\n",
       "55536  smoker who dealt with pancreatitis and cancer ...\n",
       "\n",
       "[55537 rows x 1 columns]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Read in the cleaned data, before the CountVectorizer step\n",
    "data_clean = pd.read_pickle('pickle/data_clean_r4.pkl')\n",
    "data_clean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>content</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>cancer i cancer treatment today life i disbeli...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>something link page commission work people gin...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>cluster pneumonia city member family world hea...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>airport mystery illness china credit update st...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>finding outbreak pneumonia china people family...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>55532</th>\n",
       "      <td>morning government blanket order place home co...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>55533</th>\n",
       "      <td>future talk swoop star mane side place bench w...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>55534</th>\n",
       "      <td>deal york adjacent share data coronavirus mone...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>55535</th>\n",
       "      <td>world mass life week mounting damage spread co...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>55536</th>\n",
       "      <td>smoker pancreatitis cancer reason coronavirus ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>55537 rows Ã— 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 content\n",
       "0      cancer i cancer treatment today life i disbeli...\n",
       "1      something link page commission work people gin...\n",
       "2      cluster pneumonia city member family world hea...\n",
       "3      airport mystery illness china credit update st...\n",
       "4      finding outbreak pneumonia china people family...\n",
       "...                                                  ...\n",
       "55532  morning government blanket order place home co...\n",
       "55533  future talk swoop star mane side place bench w...\n",
       "55534  deal york adjacent share data coronavirus mone...\n",
       "55535  world mass life week mounting damage spread co...\n",
       "55536  smoker pancreatitis cancer reason coronavirus ...\n",
       "\n",
       "[55537 rows x 1 columns]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Apply the nouns function to the transcripts to filter only on nouns\n",
    "data_nouns = pd.DataFrame(data_clean.content.apply(nouns))\n",
    "data_nouns.to_pickle('pickle/data_nouns.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a new document-term matrix using only nouns\n",
    "from sklearn.feature_extraction import text\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "data_nouns = pd.read_pickle('pickle/data_nouns.pkl')\n",
    "\n",
    "# Recreate a document-term matrix with only nouns\n",
    "cvn = CountVectorizer(stop_words='english')\n",
    "data_cvn = cvn.fit_transform(data_nouns.content)\n",
    "data_dtmn = pd.DataFrame(data_cvn.toarray(), columns=cvn.get_feature_names())\n",
    "data_dtmn.index = data_nouns.index\n",
    "data_dtmn.to_pickle('pickle/data_dtmn.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dtmn = pd.read_pickle('pickle/data_dtmn.pkl')\n",
    "\n",
    "# Create the gensim corpus\n",
    "corpusn = matutils.Sparse2Corpus(scipy.sparse.csr_matrix(data_dtmn.transpose()))\n",
    "\n",
    "# Create the vocabulary dictionary\n",
    "id2wordn = dict((v, k) for k, v in cvn.vocabulary_.items())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0,\n",
       "  '0.024*\"people\" + 0.023*\"coronavirus\" + 0.014*\"time\" + 0.014*\"health\" + 0.011*\"day\" + 0.011*\"newspaper\" + 0.009*\"government\" + 0.009*\"virus\" + 0.009*\"home\" + 0.007*\"order\"'),\n",
       " (1,\n",
       "  '0.018*\"coronavirus\" + 0.015*\"company\" + 0.013*\"year\" + 0.012*\"business\" + 0.012*\"economy\" + 0.011*\"market\" + 0.009*\"crisis\" + 0.009*\"week\" + 0.008*\"oil\" + 0.008*\"government\"')]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Let's start with 2 topics\n",
    "ldan = models.LdaModel(corpus=corpusn, num_topics=2, id2word=id2wordn, passes=10)\n",
    "ldan.print_topics()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0,\n",
       "  '0.023*\"coronavirus\" + 0.015*\"economy\" + 0.014*\"year\" + 0.012*\"market\" + 0.012*\"government\" + 0.011*\"crisis\" + 0.011*\"oil\" + 0.011*\"week\" + 0.010*\"business\" + 0.010*\"quarter\"'),\n",
       " (1,\n",
       "  '0.028*\"people\" + 0.025*\"coronavirus\" + 0.017*\"health\" + 0.014*\"time\" + 0.013*\"day\" + 0.011*\"virus\" + 0.011*\"government\" + 0.010*\"home\" + 0.010*\"newspaper\" + 0.007*\"world\"'),\n",
       " (2,\n",
       "  '0.016*\"company\" + 0.015*\"season\" + 0.015*\"time\" + 0.011*\"league\" + 0.011*\"newspaper\" + 0.010*\"year\" + 0.008*\"summer\" + 0.007*\"order\" + 0.007*\"business\" + 0.007*\"team\"')]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Let's try topics = 3\n",
    "ldan = models.LdaModel(corpus=corpusn, num_topics=3, id2word=id2wordn, passes=10)\n",
    "ldan.print_topics()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0,\n",
       "  '0.020*\"time\" + 0.018*\"newspaper\" + 0.012*\"season\" + 0.010*\"order\" + 0.010*\"coronavirus\" + 0.009*\"express\" + 0.009*\"archive\" + 0.009*\"league\" + 0.008*\"way\" + 0.008*\"world\"'),\n",
       " (1,\n",
       "  '0.033*\"people\" + 0.030*\"coronavirus\" + 0.022*\"health\" + 0.015*\"government\" + 0.015*\"virus\" + 0.013*\"day\" + 0.011*\"time\" + 0.009*\"home\" + 0.009*\"country\" + 0.009*\"number\"'),\n",
       " (2,\n",
       "  '0.016*\"company\" + 0.016*\"business\" + 0.015*\"economy\" + 0.015*\"year\" + 0.015*\"coronavirus\" + 0.014*\"market\" + 0.012*\"quarter\" + 0.011*\"bank\" + 0.011*\"crisis\" + 0.010*\"stock\"'),\n",
       " (3,\n",
       "  '0.022*\"oil\" + 0.022*\"coronavirus\" + 0.016*\"demand\" + 0.014*\"production\" + 0.013*\"company\" + 0.013*\"food\" + 0.012*\"china\" + 0.012*\"day\" + 0.011*\"travel\" + 0.011*\"week\"')]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Let's try 4 topics\n",
    "ldan = models.LdaModel(corpus=corpusn, num_topics=4, id2word=id2wordn, passes=10)\n",
    "ldan.print_topics()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'ldana' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-9-f1a35e1f6c05>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Let's take a look at which topics each transcript contains\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mcorpus_transformed\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mldana\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcorpusna\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0ma\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mcorpus_transformed\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata_dtmna\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'ldana' is not defined"
     ]
    }
   ],
   "source": [
    "# Let's take a look at which topics each transcript contains\n",
    "corpus_transformed = ldana[corpusna]\n",
    "list(zip([a for [(a,b)] in corpus_transformed], data_dtmna.index))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
