{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "This notebook is a part of a series of notebooks.\n",
    "1. ***Data Cleaning***\n",
    "2. Data Analysis\n",
    "3. Sentiment Analysis\n",
    "4. Topic Modelling  \n",
    "\n",
    "The purpose of this notebook is to clean the data for unecessary data, such as symbols, numbers, brackets and foreign words. The expected outcome of this data cleaning process is to have a dataframe with lemmetized english words, from english domains.\n",
    "\n",
    "# Problem statement\n",
    "How has the mainstream media's coverage of Covid-19 changed during the pandemic?\n",
    "- Can an overall change in sentiment be seen?\n",
    "- Has the topics changed over time?\n",
    "- Can a change in positivity og negativity within publishers be seen?\n",
    "\n",
    "[Link to Github repository](https://github.com/miguel2650/covid-19-research)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Getting the data\n",
    "The dataset used for this research was found at [Kaggle](https://www.kaggle.com/jannalipenkova/covid19-public-media-dataset). The dataset is open to public downloads with a verified Kaggle account. The dataset cosists of 50,000 online articles with full texts which were scraped from online media in the timespan since January 2020, focussed mainly on the non-medical aspects of COVID-19. These articles will be in all kind of languages without any form of preprocessing.  \n",
    "The authors of the dataset will continually update the datasets. However, we will focus on the latest version by the time of conduting this research.  \n",
    "*Version: covid19_articles_20200504.csv*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading covid19-public-media-dataset.zip to C:\\Users\\Mikkel\\Desktop\\covid-19-research\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  0%|          | 0.00/244M [00:00<?, ?B/s]\n",
      "  0%|          | 1.00M/244M [00:00<00:25, 9.83MB/s]\n",
      "  2%|2         | 5.00M/244M [00:00<00:20, 12.0MB/s]\n",
      "  4%|3         | 9.00M/244M [00:00<00:19, 12.4MB/s]\n",
      "  7%|7         | 18.0M/244M [00:00<00:14, 16.7MB/s]\n",
      " 10%|#         | 25.0M/244M [00:01<00:12, 18.2MB/s]\n",
      " 15%|#5        | 37.0M/244M [00:01<00:08, 24.4MB/s]\n",
      " 18%|#7        | 43.0M/244M [00:01<00:09, 22.2MB/s]\n",
      " 23%|##2       | 55.0M/244M [00:01<00:06, 29.4MB/s]\n",
      " 25%|##5       | 62.0M/244M [00:01<00:07, 26.9MB/s]\n",
      " 30%|##9       | 73.0M/244M [00:02<00:05, 30.0MB/s]\n",
      " 33%|###3      | 81.0M/244M [00:02<00:05, 32.5MB/s]\n",
      " 36%|###6      | 89.0M/244M [00:02<00:04, 35.5MB/s]\n",
      " 41%|####1     | 101M/244M [00:02<00:03, 44.9MB/s] \n",
      " 44%|####4     | 108M/244M [00:02<00:03, 44.8MB/s]\n",
      " 47%|####6     | 114M/244M [00:03<00:03, 42.5MB/s]\n",
      " 52%|#####1    | 126M/244M [00:03<00:02, 52.6MB/s]\n",
      " 55%|#####4    | 133M/244M [00:03<00:02, 48.2MB/s]\n",
      " 59%|#####8    | 143M/244M [00:03<00:01, 56.9MB/s]\n",
      " 62%|######1   | 151M/244M [00:03<00:01, 55.5MB/s]\n",
      " 65%|######4   | 158M/244M [00:03<00:01, 58.5MB/s]\n",
      " 68%|######7   | 165M/244M [00:03<00:01, 59.4MB/s]\n",
      " 71%|#######   | 172M/244M [00:03<00:01, 60.6MB/s]\n",
      " 73%|#######3  | 179M/244M [00:04<00:01, 56.2MB/s]\n",
      " 77%|#######7  | 188M/244M [00:04<00:00, 62.9MB/s]\n",
      " 82%|########2 | 200M/244M [00:04<00:00, 73.1MB/s]\n",
      " 85%|########5 | 208M/244M [00:04<00:00, 55.4MB/s]\n",
      " 88%|########8 | 215M/244M [00:04<00:00, 59.1MB/s]\n",
      " 92%|#########2| 225M/244M [00:04<00:00, 56.0MB/s]\n",
      " 96%|#########5| 233M/244M [00:05<00:00, 52.6MB/s]\n",
      "100%|##########| 244M/244M [00:05<00:00, 50.1MB/s]\n"
     ]
    }
   ],
   "source": [
    "import os.path\n",
    "# Retrieves the dataset from https://www.kaggle.com/jannalipenkova/covid19-public-media-dataset\n",
    "# In order to do so, please go to your kaggle account and create an API key.\n",
    "# Follow the documentation here https://github.com/Kaggle/kaggle-api to set it up.\n",
    "!kaggle datasets download -d jannalipenkova/covid19-public-media-dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from zipfile import ZipFile\n",
    "\n",
    "# Creates a ZipFile Object and loads the dataset into it\n",
    "with ZipFile('covid19-public-media-dataset.zip', 'r') as zipObj:\n",
    "    # Extracts all the contents of the zip file in to the datasets folder.\n",
    "    zipObj.extractall('datasets')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>date</th>\n",
       "      <th>domain</th>\n",
       "      <th>url</th>\n",
       "      <th>author</th>\n",
       "      <th>content</th>\n",
       "      <th>topic_area</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>My experience of surviving cancer twice</td>\n",
       "      <td>2020-01-03</td>\n",
       "      <td>medicalnewstoday</td>\n",
       "      <td>https://www.medicalnewstoday.com/articles/327373</td>\n",
       "      <td>Helen Ziatyk</td>\n",
       "      <td>“Helen, I’m so sorry to tell you that you have stage 4 ovarian cancer.” I will never forget hearing those words. Cancer treatment was pretty gruel...</td>\n",
       "      <td>healthcare</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Ginger: Health benefits and dietary tips</td>\n",
       "      <td>2020-01-03</td>\n",
       "      <td>medicalnewstoday</td>\n",
       "      <td>https://www.medicalnewstoday.com/articles/265990.php</td>\n",
       "      <td>Jenna Fletcher</td>\n",
       "      <td>If you buy something through a link on this page, we may earn a small commission. How this works. People have used ginger in cooking and medicine ...</td>\n",
       "      <td>healthcare</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                      title        date            domain  \\\n",
       "0   My experience of surviving cancer twice  2020-01-03  medicalnewstoday   \n",
       "1  Ginger: Health benefits and dietary tips  2020-01-03  medicalnewstoday   \n",
       "\n",
       "                                                    url          author  \\\n",
       "0      https://www.medicalnewstoday.com/articles/327373    Helen Ziatyk   \n",
       "1  https://www.medicalnewstoday.com/articles/265990.php  Jenna Fletcher   \n",
       "\n",
       "                                                                                                                                                 content  \\\n",
       "0  “Helen, I’m so sorry to tell you that you have stage 4 ovarian cancer.” I will never forget hearing those words. Cancer treatment was pretty gruel...   \n",
       "1  If you buy something through a link on this page, we may earn a small commission. How this works. People have used ginger in cooking and medicine ...   \n",
       "\n",
       "   topic_area  \n",
       "0  healthcare  \n",
       "1  healthcare  "
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Sets the size of width when printing our the dataframe.\n",
    "pd.set_option('max_colwidth', 150)\n",
    "# Reading the CSV file intro a Pandas dataframe.\n",
    "data_df = pd.read_csv('datasets/covid19_articles_20200504.csv', index_col=0)\n",
    "data_df.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cleaning round 1\n",
    "The most simple round of data cleaning. This will makre sure that the data will only contain one long line of words in each article."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply first round of text cleaning\n",
    "import re\n",
    "import string\n",
    "\n",
    "def clean_text_round1(text):\n",
    "    '''Make text lowercase, remove text in square brackets, remove punctuation and remove words containing numbers.'''\n",
    "    text = text.lower()\n",
    "    text = re.sub('\\[.*?\\]', '', text)\n",
    "    text = re.sub('[%s]' % re.escape(string.punctuation), '', text)\n",
    "    text = re.sub('\\w*\\d\\w*', '', text)\n",
    "    return text\n",
    "\n",
    "round1 = lambda x: clean_text_round1(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>content</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>“helen i’m so sorry to tell you that you have stage  ovarian cancer” i will never forget hearing those words cancer treatment was pretty grueling ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>if you buy something through a link on this page we may earn a small commission how this works people have used ginger in cooking and medicine sin...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>a cluster of more than  pneumonia cases in the central chinese city of wuhan may be due to a newly emerging member of the family of viruses that c...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>passengers arriving at hong kongs international airport are being monitored for signs a mystery illness that emerged in central china credit andy ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>the finding that the outbreak of viral pneumonia in china that has struck  people may be caused by a coronavirus the family of viruses behind sars...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                                                                 content\n",
       "0  “helen i’m so sorry to tell you that you have stage  ovarian cancer” i will never forget hearing those words cancer treatment was pretty grueling ...\n",
       "1  if you buy something through a link on this page we may earn a small commission how this works people have used ginger in cooking and medicine sin...\n",
       "2  a cluster of more than  pneumonia cases in the central chinese city of wuhan may be due to a newly emerging member of the family of viruses that c...\n",
       "3  passengers arriving at hong kongs international airport are being monitored for signs a mystery illness that emerged in central china credit andy ...\n",
       "4  the finding that the outbreak of viral pneumonia in china that has struck  people may be caused by a coronavirus the family of viruses behind sars..."
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_clean = pd.DataFrame(data_df.content.apply(round1))\n",
    "data_clean.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cleaning round 2\n",
    "Some of the symbols and punctuation was not removed the first time around. This is the 2. round of cleaning to make sure it is completely removed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply a second round of cleaning\n",
    "def clean_text_round2(text):\n",
    "    '''Get rid of some additional punctuation and symbols that was missed the first time around.'''\n",
    "    text = re.sub('[‘’“”…]', '', text)\n",
    "    text = re.sub('\\n', '', text)\n",
    "    return text\n",
    "\n",
    "round2 = lambda x: clean_text_round2(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>content</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>helen im so sorry to tell you that you have stage  ovarian cancer i will never forget hearing those words cancer treatment was pretty grueling in ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>if you buy something through a link on this page we may earn a small commission how this works people have used ginger in cooking and medicine sin...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>a cluster of more than  pneumonia cases in the central chinese city of wuhan may be due to a newly emerging member of the family of viruses that c...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>passengers arriving at hong kongs international airport are being monitored for signs a mystery illness that emerged in central china credit andy ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>the finding that the outbreak of viral pneumonia in china that has struck  people may be caused by a coronavirus the family of viruses behind sars...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                                                                 content\n",
       "0  helen im so sorry to tell you that you have stage  ovarian cancer i will never forget hearing those words cancer treatment was pretty grueling in ...\n",
       "1  if you buy something through a link on this page we may earn a small commission how this works people have used ginger in cooking and medicine sin...\n",
       "2  a cluster of more than  pneumonia cases in the central chinese city of wuhan may be due to a newly emerging member of the family of viruses that c...\n",
       "3  passengers arriving at hong kongs international airport are being monitored for signs a mystery illness that emerged in central china credit andy ...\n",
       "4  the finding that the outbreak of viral pneumonia in china that has struck  people may be caused by a coronavirus the family of viruses behind sars..."
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_clean = pd.DataFrame(data_clean.content.apply(round2))\n",
    "data_clean.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cleaning round 3\n",
    "After cleaning round 1 and 2, the data is clean for any kind of symbols and only contains words now.  \n",
    "This time around, the data will be cleaned for any words that does not appear to be in english. This will make it easier for us to understand and work with the data. At the same time, the same words wont appear in diffrent languages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package words to\n",
      "[nltk_data]     C:\\Users\\Mikkel\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package words is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk, re\n",
    "# Using the Natural Language Toolkit to clean the data for non-english words.\n",
    "nltk.download('words')\n",
    "\n",
    "def clean_text_round3(text):\n",
    "    '''Remove all non-English word except for words containing \"corona\" and \"covid\"'''\n",
    "    covid_reg = re.compile(r'.*corona.*|.*covid.*', re.IGNORECASE)\n",
    "    \n",
    "    # Creating a set containing all the words from the NLTK library.\n",
    "    words = set(nltk.corpus.words.words())\n",
    "\n",
    "    # wordpunct_tokenize will split all words.\n",
    "    return \" \".join(w for w in nltk.wordpunct_tokenize(text) \\\n",
    "             if w.lower() in words or covid_reg.match(w))\n",
    "\n",
    "round3 = lambda x: clean_text_round3(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_clean = pd.DataFrame(data_clean.content.apply(round3))\n",
    "# Creating a folder for pickle files\n",
    "if not os.path.exists(\"pickle/\"):\n",
    "    !mkdir \"pickle\"\n",
    "# Saving the cleaned data as a pickle file.\n",
    "data_clean = pd.read_pickle(\"pickle/data_clean_r3.pkl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cleaning round 4\n",
    "Now that the data only contains english words, it will be possible to clean the words for lemmatization. This will prevent the same words from being repeated in diffrent comparison."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\Mikkel\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "# Using the Natural Language Toolkit to lemmatize the data.\n",
    "nltk.download('wordnet')\n",
    "\n",
    "def clean_text_round4(text):\n",
    "    '''Lemmatize words except for \"was\" as we experienced the issue that it would return \"wa\"'''\n",
    "    lemmer=WordNetLemmatizer()\n",
    "    \n",
    "    return ' '.join([lemmer.lemmatize(word) for word in nltk.wordpunct_tokenize(text) if word not in 'was'])\n",
    "\n",
    "round4 = lambda x: clean_text_round4(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-14-093811a0110a>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mdata_clean\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata_clean\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcontent\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mround4\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mc:\\users\\mikkel\\datascienceenv\\datascienceenv\\lib\\site-packages\\pandas\\core\\series.py\u001b[0m in \u001b[0;36mapply\u001b[1;34m(self, func, convert_dtype, args, **kwds)\u001b[0m\n\u001b[0;32m   4043\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   4044\u001b[0m                 \u001b[0mvalues\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mobject\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 4045\u001b[1;33m                 \u001b[0mmapped\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlib\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmap_infer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mconvert\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mconvert_dtype\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   4046\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   4047\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmapped\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmapped\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mSeries\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mpandas/_libs/lib.pyx\u001b[0m in \u001b[0;36mpandas._libs.lib.map_infer\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32m<ipython-input-13-31a8344c3b71>\u001b[0m in \u001b[0;36m<lambda>\u001b[1;34m(x)\u001b[0m\n\u001b[0;32m     10\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[1;34m' '\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mlemmer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlemmatize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mword\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mword\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mnltk\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwordpunct_tokenize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mword\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32min\u001b[0m \u001b[1;34m'was'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 12\u001b[1;33m \u001b[0mround4\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mclean_text_round4\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-13-31a8344c3b71>\u001b[0m in \u001b[0;36mclean_text_round4\u001b[1;34m(text)\u001b[0m\n\u001b[0;32m      8\u001b[0m     \u001b[0mlemmer\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mWordNetLemmatizer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 10\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[1;34m' '\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mlemmer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlemmatize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mword\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mword\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mnltk\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwordpunct_tokenize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mword\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32min\u001b[0m \u001b[1;34m'was'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     11\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     12\u001b[0m \u001b[0mround4\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mclean_text_round4\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-13-31a8344c3b71>\u001b[0m in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m      8\u001b[0m     \u001b[0mlemmer\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mWordNetLemmatizer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 10\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[1;34m' '\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mlemmer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlemmatize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mword\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mword\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mnltk\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwordpunct_tokenize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mword\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32min\u001b[0m \u001b[1;34m'was'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     11\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     12\u001b[0m \u001b[0mround4\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mclean_text_round4\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\mikkel\\datascienceenv\\datascienceenv\\lib\\site-packages\\nltk\\stem\\wordnet.py\u001b[0m in \u001b[0;36mlemmatize\u001b[1;34m(self, word, pos)\u001b[0m\n\u001b[0;32m     36\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     37\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mlemmatize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mword\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpos\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mNOUN\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 38\u001b[1;33m         \u001b[0mlemmas\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mwordnet\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_morphy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mword\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpos\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     39\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mmin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlemmas\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mlemmas\u001b[0m \u001b[1;32melse\u001b[0m \u001b[0mword\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     40\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\mikkel\\datascienceenv\\datascienceenv\\lib\\site-packages\\nltk\\corpus\\reader\\wordnet.py\u001b[0m in \u001b[0;36m_morphy\u001b[1;34m(self, form, pos, check_exceptions)\u001b[0m\n\u001b[0;32m   1899\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1900\u001b[0m         \u001b[1;31m# 1. Apply rules once to the input to get y1, y2, y3, etc.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1901\u001b[1;33m         \u001b[0mforms\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mapply_rules\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mform\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1902\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1903\u001b[0m         \u001b[1;31m# 2. Return all that are in the database (and check the original too)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\mikkel\\datascienceenv\\datascienceenv\\lib\\site-packages\\nltk\\corpus\\reader\\wordnet.py\u001b[0m in \u001b[0;36mapply_rules\u001b[1;34m(forms)\u001b[0m\n\u001b[0;32m   1875\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1876\u001b[0m         \u001b[1;32mdef\u001b[0m \u001b[0mapply_rules\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mforms\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1877\u001b[1;33m             return [\n\u001b[0m\u001b[0;32m   1878\u001b[0m                 \u001b[0mform\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;33m-\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mold\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mnew\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1879\u001b[0m                 \u001b[1;32mfor\u001b[0m \u001b[0mform\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mforms\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\mikkel\\datascienceenv\\datascienceenv\\lib\\site-packages\\nltk\\corpus\\reader\\wordnet.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m   1878\u001b[0m                 \u001b[0mform\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;33m-\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mold\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mnew\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1879\u001b[0m                 \u001b[1;32mfor\u001b[0m \u001b[0mform\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mforms\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1880\u001b[1;33m                 \u001b[1;32mfor\u001b[0m \u001b[0mold\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnew\u001b[0m \u001b[1;32min\u001b[0m \u001b[0msubstitutions\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1881\u001b[0m                 \u001b[1;32mif\u001b[0m \u001b[0mform\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mendswith\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mold\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1882\u001b[0m             ]\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "data_clean = pd.DataFrame(data_clean.content.apply(round4))\n",
    "data_clean.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# End of cleaning\n",
    "This is the end of the data cleaning. More steps could have been applied however, a decision were made that the data was clean enough for analysis. The cleaned data will be saved as files for analysis in the next notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saves the cleaned data at pickle file.\n",
    "if not os.path.exists(\"pickle/\"):\n",
    "    !mkdir \"pickle\"\n",
    "# Saving the cleaned data\n",
    "data_clean.to_pickle(\"pickle/data_clean_r4.pkl\")\n",
    "# Saving the raw data\n",
    "data_df.to_pickle(\"pickle/data_df.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
